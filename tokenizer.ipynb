{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5edd7534",
   "metadata": {},
   "source": [
    "# MusicXML Tokenizer v0 — Baseline Scaffold\n",
    "\n",
    "**Goal:** Minimal working tokenizer + detokenizer + evaluation harness for SymbTr v3 Turkish makam MusicXML files.\n",
    "\n",
    "**Token types:** `<BOS>`, `<EOS>`, `PART_`, `TIME_SIG_`, `KEY_`, `CLEF_`, `BAR_`, `POS_BAR_`, `POS_ABS_`, `PITCH_`, `DUR_`, `REST`\n",
    "\n",
    "**v0 limitations (to be fixed in later iterations):**\n",
    "- Pitch: uses `<step>` + `<octave>` only — microtonal `<alter>` values are ignored\n",
    "- No tie, grace note, repeat, fermata, lyric, or dynamics tokens\n",
    "- Duration computed from raw `<duration>/<divisions>` only\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Setup and Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9d5b049",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Install dependencies (for Google Colab) ──\n",
    "# Uncomment the following lines if running on Colab:\n",
    "# !pip install lxml music21 python-Levenshtein pandas matplotlib\n",
    "\n",
    "import os\n",
    "import glob\n",
    "import random\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "from collections import Counter, defaultdict\n",
    "from fractions import Fraction\n",
    "\n",
    "from lxml import etree\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Suppress music21 warnings for cleaner output\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "\n",
    "# We import music21 for the detokenizer and evaluation only\n",
    "import music21\n",
    "\n",
    "# Reproducibility\n",
    "RANDOM_SEED = 42\n",
    "random.seed(RANDOM_SEED)\n",
    "\n",
    "print(f\"lxml version: {etree.LXML_VERSION}\")\n",
    "print(f\"music21 version: {music21.VERSION_STR}\")\n",
    "print(\"Setup complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b020f748",
   "metadata": {},
   "source": [
    "## 2. Load Dataset File Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4485a8e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Load all XML file paths from xml_v3/ ──\n",
    "XML_DIR = Path(\"xml_v3\")\n",
    "\n",
    "# Also support Colab path structure\n",
    "if not XML_DIR.exists():\n",
    "    # Try relative to notebook location\n",
    "    XML_DIR = Path(\"/content/musicxml_tokenizer/xml_v3\")\n",
    "\n",
    "all_xml_files = sorted(XML_DIR.glob(\"*.xml\"))\n",
    "print(f\"Total XML files found: {len(all_xml_files)}\")\n",
    "print(f\"\\nFirst 5 files:\")\n",
    "for f in all_xml_files[:5]:\n",
    "    print(f\"  {f.name}\")\n",
    "print(f\"\\nLast 5 files:\")\n",
    "for f in all_xml_files[-5:]:\n",
    "    print(f\"  {f.name}\")\n",
    "\n",
    "# Select a random 100-file sample for evaluation\n",
    "EVAL_SAMPLE_SIZE = 100\n",
    "eval_sample = random.sample(all_xml_files, min(EVAL_SAMPLE_SIZE, len(all_xml_files)))\n",
    "print(f\"\\nEvaluation sample size: {len(eval_sample)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5194e07",
   "metadata": {},
   "source": [
    "## 3. Parse MusicXML Structure with lxml\n",
    "\n",
    "Explore the XML tree structure of a sample file to confirm the parsing approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58491a0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Explore MusicXML structure of a sample file ──\n",
    "sample_file = all_xml_files[0]\n",
    "print(f\"Exploring: {sample_file.name}\\n\")\n",
    "\n",
    "tree = etree.parse(str(sample_file))\n",
    "root = tree.getroot()\n",
    "\n",
    "# Show top-level structure\n",
    "print(\"Root tag:\", root.tag)\n",
    "print(\"Top-level children:\")\n",
    "for child in root:\n",
    "    print(f\"  <{child.tag}>\")\n",
    "\n",
    "# Show part-list → part names\n",
    "part_list = root.find(\"part-list\")\n",
    "if part_list is not None:\n",
    "    for sp in part_list.findall(\"score-part\"):\n",
    "        pid = sp.get(\"id\")\n",
    "        pname = sp.findtext(\"part-name\", default=\"?\")\n",
    "        print(f\"\\n  Part ID={pid}, Name='{pname}'\")\n",
    "\n",
    "# Show first measure structure\n",
    "parts = root.findall(\"part\")\n",
    "print(f\"\\nNumber of parts: {len(parts)}\")\n",
    "first_part = parts[0]\n",
    "first_measure = first_part.find(\"measure\")\n",
    "print(f\"\\nFirst measure (number={first_measure.get('number')}) children:\")\n",
    "for child in first_measure:\n",
    "    print(f\"  <{child.tag}>\", {k: v for k, v in child.attrib.items()} if child.attrib else \"\")\n",
    "    for sub in child:\n",
    "        txt = sub.text.strip() if sub.text and sub.text.strip() else \"\"\n",
    "        print(f\"    <{sub.tag}> {txt}\")\n",
    "\n",
    "# Show divisions\n",
    "attrs = first_measure.find(\"attributes\")\n",
    "if attrs is not None:\n",
    "    div = attrs.findtext(\"divisions\")\n",
    "    print(f\"\\nDivisions per quarter note: {div}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf4bb84e",
   "metadata": {},
   "source": [
    "## 4. Helper Functions for Pitch, Duration, Key, and Time Signature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfcb3625",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Helper functions for tokenization ──\n",
    "\n",
    "def get_part_name(root, part_id):\n",
    "    \"\"\"Look up part name from <part-list> given a part ID.\"\"\"\n",
    "    part_list = root.find(\"part-list\")\n",
    "    if part_list is not None:\n",
    "        for sp in part_list.findall(\"score-part\"):\n",
    "            if sp.get(\"id\") == part_id:\n",
    "                name = sp.findtext(\"part-name\", default=\"\")\n",
    "                # Also try instrument-name as fallback\n",
    "                if not name or name.strip() == \"\":\n",
    "                    instr = sp.find(\".//instrument-name\")\n",
    "                    if instr is not None and instr.text:\n",
    "                        name = instr.text\n",
    "                return name.strip() if name else \"Unknown\"\n",
    "    return \"Unknown\"\n",
    "\n",
    "\n",
    "def extract_pitch(note_elem):\n",
    "    \"\"\"Extract pitch from a <note> element. Returns 'PITCH_<step><octave>' string.\n",
    "    v0: ignores <alter> (microtonal inflection).\"\"\"\n",
    "    pitch_elem = note_elem.find(\"pitch\")\n",
    "    if pitch_elem is None:\n",
    "        return None\n",
    "    step = pitch_elem.findtext(\"step\", default=\"C\")\n",
    "    octave = pitch_elem.findtext(\"octave\", default=\"4\")\n",
    "    return f\"PITCH_{step}{octave}\"\n",
    "\n",
    "\n",
    "def extract_duration_ql(note_elem, divisions):\n",
    "    \"\"\"Convert <duration> to quarter-note length using divisions.\n",
    "    Returns a float representing quarter-note duration.\"\"\"\n",
    "    dur_text = note_elem.findtext(\"duration\")\n",
    "    if dur_text is None:\n",
    "        return 0.0\n",
    "    duration = int(dur_text)\n",
    "    if divisions == 0:\n",
    "        return 0.0\n",
    "    ql = duration / divisions\n",
    "    return round(ql, 6)  # Avoid floating point noise\n",
    "\n",
    "\n",
    "def is_rest(note_elem):\n",
    "    \"\"\"Check if a <note> element is a rest.\"\"\"\n",
    "    return note_elem.find(\"rest\") is not None\n",
    "\n",
    "\n",
    "def is_grace_note(note_elem):\n",
    "    \"\"\"Check if a <note> element is a grace note (no duration contribution).\"\"\"\n",
    "    return note_elem.find(\"grace\") is not None\n",
    "\n",
    "\n",
    "def extract_key_signature(key_elem):\n",
    "    \"\"\"Parse a <key> element and return a KEY_ token string.\n",
    "\n",
    "    Handles two formats:\n",
    "    1. Standard: <fifths> + <mode>\n",
    "    2. Non-standard (SymbTr): <key-step> + <key-alter> + <key-accidental>\n",
    "\n",
    "    For v0, non-standard keys are represented as KEY_<step>_<accidental>.\n",
    "    \"\"\"\n",
    "    if key_elem is None:\n",
    "        return None\n",
    "\n",
    "    # Try standard format first\n",
    "    fifths = key_elem.findtext(\"fifths\")\n",
    "    if fifths is not None:\n",
    "        fifths_int = int(fifths)\n",
    "        mode = key_elem.findtext(\"mode\", default=\"major\")\n",
    "        # Map fifths to tonic\n",
    "        major_keys = [\"C\", \"G\", \"D\", \"A\", \"E\", \"B\", \"F#\",\n",
    "                       \"Gb\", \"Db\", \"Ab\", \"Eb\", \"Bb\", \"F\"]\n",
    "        minor_keys = [\"A\", \"E\", \"B\", \"F#\", \"C#\", \"G#\", \"D#\",\n",
    "                       \"Eb\", \"Bb\", \"F\", \"C\", \"G\", \"D\"]\n",
    "        if mode == \"minor\":\n",
    "            tonic = minor_keys[fifths_int] if -6 <= fifths_int <= 6 else \"A\"\n",
    "        else:\n",
    "            tonic = major_keys[fifths_int] if -6 <= fifths_int <= 6 else \"C\"\n",
    "        return f\"KEY_{tonic}_{mode}\"\n",
    "\n",
    "    # Non-standard SymbTr format: one or more <key-step> + <key-alter> pairs\n",
    "    key_steps = key_elem.findall(\"key-step\")\n",
    "    key_alters = key_elem.findall(\"key-alter\")\n",
    "    key_accidentals = key_elem.findall(\"key-accidental\")\n",
    "\n",
    "    if key_steps:\n",
    "        # Build a composite key signature description\n",
    "        parts = []\n",
    "        for i, step_elem in enumerate(key_steps):\n",
    "            step = step_elem.text\n",
    "            alter = key_alters[i].text if i < len(key_alters) else \"0\"\n",
    "            accidental = key_accidentals[i].text if i < len(key_accidentals) else \"\"\n",
    "            if accidental:\n",
    "                parts.append(f\"{step}-{accidental}\")\n",
    "            else:\n",
    "                parts.append(f\"{step}({alter})\")\n",
    "        # Return composite key signature\n",
    "        return \"KEY_\" + \"_\".join(parts)\n",
    "\n",
    "    return None\n",
    "\n",
    "\n",
    "def extract_time_signature(time_elem):\n",
    "    \"\"\"Parse a <time> element and return 'TIME_SIG_<beats>/<beat-type>'.\"\"\"\n",
    "    if time_elem is None:\n",
    "        return None\n",
    "    beats = time_elem.findtext(\"beats\", default=\"4\")\n",
    "    beat_type = time_elem.findtext(\"beat-type\", default=\"4\")\n",
    "    return f\"TIME_SIG_{beats}/{beat_type}\"\n",
    "\n",
    "\n",
    "def format_position(pos):\n",
    "    \"\"\"Format a position value to a clean string.\n",
    "    Use integer representation if it's a whole number, otherwise float.\"\"\"\n",
    "    if pos == int(pos):\n",
    "        return str(int(pos)) + \".0\"\n",
    "    else:\n",
    "        return f\"{pos:.4f}\".rstrip('0').rstrip('.')\n",
    "        # Ensure at least one decimal place\n",
    "        result = f\"{pos:.4f}\".rstrip('0')\n",
    "        if result.endswith('.'):\n",
    "            result += '0'\n",
    "        return result\n",
    "\n",
    "\n",
    "# Quick test of helpers\n",
    "print(\"Helper functions defined successfully.\")\n",
    "print(f\"format_position(0) = '{format_position(0)}'\")\n",
    "print(f\"format_position(1.5) = '{format_position(1.5)}'\")\n",
    "print(f\"format_position(0.25) = '{format_position(0.25)}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6e02308",
   "metadata": {},
   "source": [
    "## 5. Implement `tokenize(xml_path) → list[str]`\n",
    "\n",
    "The main tokenizer function. Parses MusicXML with `lxml`, walks the tree, and emits a flat list of string tokens.\n",
    "\n",
    "**Partwise tokenization:** Each part's bars are emitted contiguously under a `PART_` header before moving to the next part."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0934b0f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(xml_path):\n",
    "    \"\"\"Tokenize a MusicXML file into a list of string tokens.\n",
    "\n",
    "    Args:\n",
    "        xml_path: Path to a MusicXML (.xml) file.\n",
    "\n",
    "    Returns:\n",
    "        List of string tokens representing the score.\n",
    "    \"\"\"\n",
    "    tree = etree.parse(str(xml_path))\n",
    "    root = tree.getroot()\n",
    "\n",
    "    tokens = [\"<BOS>\"]\n",
    "\n",
    "    # Iterate over each part (partwise tokenization)\n",
    "    for part_elem in root.findall(\"part\"):\n",
    "        part_id = part_elem.get(\"id\")\n",
    "        part_name = get_part_name(root, part_id)\n",
    "        tokens.append(f\"PART_{part_name}\")\n",
    "\n",
    "        # State tracking for this part\n",
    "        current_time_sig = None\n",
    "        current_key = None\n",
    "        clef_emitted = False\n",
    "        divisions = 1  # default, will be set from <attributes>\n",
    "        pos_abs = 0.0  # absolute position in quarter notes\n",
    "\n",
    "        for measure in part_elem.findall(\"measure\"):\n",
    "            measure_number = measure.get(\"number\", \"0\")\n",
    "\n",
    "            # ── Process <attributes> for key, time, clef, divisions ──\n",
    "            for attrs in measure.findall(\"attributes\"):\n",
    "                # Update divisions if present\n",
    "                div_text = attrs.findtext(\"divisions\")\n",
    "                if div_text is not None:\n",
    "                    divisions = int(div_text)\n",
    "\n",
    "                # Time signature\n",
    "                time_elem = attrs.find(\"time\")\n",
    "                if time_elem is not None:\n",
    "                    new_time_sig = extract_time_signature(time_elem)\n",
    "                    if new_time_sig and new_time_sig != current_time_sig:\n",
    "                        tokens.append(new_time_sig)\n",
    "                        current_time_sig = new_time_sig\n",
    "\n",
    "                # Key signature\n",
    "                key_elem = attrs.find(\"key\")\n",
    "                if key_elem is not None:\n",
    "                    new_key = extract_key_signature(key_elem)\n",
    "                    if new_key and new_key != current_key:\n",
    "                        tokens.append(new_key)\n",
    "                        current_key = new_key\n",
    "\n",
    "                # Clef (emit default if not yet emitted)\n",
    "                clef_elem = attrs.find(\"clef\")\n",
    "                if clef_elem is not None:\n",
    "                    clef_sign = clef_elem.findtext(\"sign\", default=\"G\")\n",
    "                    clef_line = clef_elem.findtext(\"line\", default=\"2\")\n",
    "                    tokens.append(f\"CLEF_{clef_sign}_{clef_line}\")\n",
    "                    clef_emitted = True\n",
    "\n",
    "            # Emit default clef if none found (dataset typically lacks <clef>)\n",
    "            if not clef_emitted:\n",
    "                tokens.append(\"CLEF_G_2\")\n",
    "                clef_emitted = True\n",
    "\n",
    "            # ── BAR token ──\n",
    "            tokens.append(f\"BAR_{measure_number}\")\n",
    "\n",
    "            # ── Process notes and rests ──\n",
    "            pos_bar = 0.0  # position within current bar\n",
    "\n",
    "            for elem in measure:\n",
    "                if elem.tag == \"note\":\n",
    "                    # Skip grace notes in v0 (they have no <duration>)\n",
    "                    if is_grace_note(elem):\n",
    "                        continue\n",
    "\n",
    "                    # Get duration in quarter-note length\n",
    "                    dur_ql = extract_duration_ql(elem, divisions)\n",
    "\n",
    "                    # Emit position tokens\n",
    "                    tokens.append(f\"POS_BAR_{format_position(pos_bar)}\")\n",
    "                    tokens.append(f\"POS_ABS_{format_position(pos_abs)}\")\n",
    "\n",
    "                    if is_rest(elem):\n",
    "                        # Rest token\n",
    "                        tokens.append(\"REST\")\n",
    "                        tokens.append(f\"DUR_{format_position(dur_ql)}\")\n",
    "                    else:\n",
    "                        # Pitch token\n",
    "                        pitch_token = extract_pitch(elem)\n",
    "                        if pitch_token:\n",
    "                            tokens.append(pitch_token)\n",
    "                            tokens.append(f\"DUR_{format_position(dur_ql)}\")\n",
    "\n",
    "                    # Advance positions\n",
    "                    pos_bar += dur_ql\n",
    "                    pos_abs += dur_ql\n",
    "\n",
    "                elif elem.tag == \"forward\":\n",
    "                    # <forward> advances the position without a note\n",
    "                    dur_text = elem.findtext(\"duration\")\n",
    "                    if dur_text:\n",
    "                        fwd_ql = int(dur_text) / divisions\n",
    "                        pos_bar += fwd_ql\n",
    "                        pos_abs += fwd_ql\n",
    "\n",
    "                elif elem.tag == \"backup\":\n",
    "                    # <backup> moves the position backwards\n",
    "                    dur_text = elem.findtext(\"duration\")\n",
    "                    if dur_text:\n",
    "                        bak_ql = int(dur_text) / divisions\n",
    "                        pos_bar -= bak_ql\n",
    "                        pos_abs -= bak_ql\n",
    "\n",
    "    tokens.append(\"<EOS>\")\n",
    "    return tokens\n",
    "\n",
    "\n",
    "# ── Quick test ──\n",
    "test_file = all_xml_files[0]\n",
    "print(f\"Tokenizing: {test_file.name}\\n\")\n",
    "test_tokens = tokenize(test_file)\n",
    "print(f\"Total tokens: {len(test_tokens)}\")\n",
    "print(f\"\\nFirst 40 tokens:\")\n",
    "for i, t in enumerate(test_tokens[:40]):\n",
    "    print(f\"  [{i:3d}] {t}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "126ec47d",
   "metadata": {},
   "source": [
    "## 6. Implement `detokenize(tokens) → music21.Score`\n",
    "\n",
    "Reconstructs a `music21.stream.Score` from a list of tokens. Used for round-trip evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8025c803",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detokenize(tokens):\n",
    "    \"\"\"Reconstruct a music21.Score from a list of tokens.\n",
    "\n",
    "    Args:\n",
    "        tokens: List of string tokens (as produced by tokenize()).\n",
    "\n",
    "    Returns:\n",
    "        music21.stream.Score object.\n",
    "    \"\"\"\n",
    "    score = music21.stream.Score()\n",
    "    current_part = None\n",
    "    current_measure = None\n",
    "    current_measure_num = 0\n",
    "    current_time_sig = None\n",
    "    current_key_sig = None\n",
    "    pending_pitch = None  # Holds a PITCH_ token waiting for its DUR_\n",
    "\n",
    "    i = 0\n",
    "    while i < len(tokens):\n",
    "        token = tokens[i]\n",
    "\n",
    "        if token == \"<BOS>\" or token == \"<EOS>\":\n",
    "            i += 1\n",
    "            continue\n",
    "\n",
    "        elif token.startswith(\"PART_\"):\n",
    "            # Finalize previous part\n",
    "            if current_measure is not None and current_part is not None:\n",
    "                current_part.append(current_measure)\n",
    "                current_measure = None\n",
    "            if current_part is not None:\n",
    "                score.append(current_part)\n",
    "\n",
    "            part_name = token[5:]  # Strip \"PART_\"\n",
    "            current_part = music21.stream.Part()\n",
    "            current_part.partName = part_name\n",
    "            current_measure = None\n",
    "            current_time_sig = None\n",
    "            current_key_sig = None\n",
    "            i += 1\n",
    "\n",
    "        elif token.startswith(\"TIME_SIG_\"):\n",
    "            ts_str = token[9:]  # e.g., \"4/4\" or \"9/8\"\n",
    "            num, denom = ts_str.split(\"/\")\n",
    "            ts = music21.meter.TimeSignature(f\"{num}/{denom}\")\n",
    "            current_time_sig = ts\n",
    "            # Append to current measure or part\n",
    "            if current_measure is not None:\n",
    "                current_measure.append(ts)\n",
    "            elif current_part is not None:\n",
    "                # Time sig before first bar — will be added to first measure\n",
    "                current_time_sig = ts\n",
    "            i += 1\n",
    "\n",
    "        elif token.startswith(\"KEY_\"):\n",
    "            key_str = token[4:]  # e.g., \"C_major\" or \"B-quarter-flat\"\n",
    "            # For v0 standard keys, try to parse; for non-standard, store as-is\n",
    "            try:\n",
    "                parts = key_str.split(\"_\")\n",
    "                if len(parts) == 2 and parts[1] in (\"major\", \"minor\"):\n",
    "                    ks = music21.key.Key(parts[0], parts[1])\n",
    "                else:\n",
    "                    # Non-standard key — store as KeySignature with 0 sharps\n",
    "                    ks = music21.key.KeySignature(0)\n",
    "                current_key_sig = ks\n",
    "                if current_measure is not None:\n",
    "                    current_measure.append(ks)\n",
    "            except Exception:\n",
    "                pass\n",
    "            i += 1\n",
    "\n",
    "        elif token.startswith(\"CLEF_\"):\n",
    "            parts = token[5:].split(\"_\")\n",
    "            sign = parts[0] if len(parts) >= 1 else \"G\"\n",
    "            line = int(parts[1]) if len(parts) >= 2 else 2\n",
    "            if sign == \"G\" and line == 2:\n",
    "                clef = music21.clef.TrebleClef()\n",
    "            elif sign == \"F\" and line == 4:\n",
    "                clef = music21.clef.BassClef()\n",
    "            elif sign == \"C\" and line == 3:\n",
    "                clef = music21.clef.AltoClef()\n",
    "            else:\n",
    "                clef = music21.clef.Clef()\n",
    "                clef.sign = sign\n",
    "                clef.line = line\n",
    "            if current_measure is not None:\n",
    "                current_measure.append(clef)\n",
    "            elif current_part is not None:\n",
    "                # Will add to first measure\n",
    "                pass\n",
    "            i += 1\n",
    "\n",
    "        elif token.startswith(\"BAR_\"):\n",
    "            # Finalize previous measure\n",
    "            if current_measure is not None and current_part is not None:\n",
    "                current_part.append(current_measure)\n",
    "\n",
    "            bar_num = int(token[4:])\n",
    "            current_measure = music21.stream.Measure(number=bar_num)\n",
    "\n",
    "            # Add time signature and key to first measure if pending\n",
    "            if bar_num == 1 or (current_time_sig and bar_num == 1):\n",
    "                if current_time_sig:\n",
    "                    current_measure.append(current_time_sig)\n",
    "                    current_time_sig = None\n",
    "                if current_key_sig:\n",
    "                    current_measure.append(current_key_sig)\n",
    "                    current_key_sig = None\n",
    "            i += 1\n",
    "\n",
    "        elif token.startswith(\"POS_BAR_\") or token.startswith(\"POS_ABS_\"):\n",
    "            # Position tokens — skip for now (reconstruction uses sequential append)\n",
    "            i += 1\n",
    "\n",
    "        elif token.startswith(\"PITCH_\"):\n",
    "            # Store pitch, wait for DUR_ token\n",
    "            pending_pitch = token[6:]  # e.g., \"C5\" or \"A4\"\n",
    "            i += 1\n",
    "\n",
    "        elif token == \"REST\":\n",
    "            pending_pitch = \"REST\"\n",
    "            i += 1\n",
    "\n",
    "        elif token.startswith(\"DUR_\"):\n",
    "            dur_str = token[4:]\n",
    "            try:\n",
    "                dur_ql = float(dur_str)\n",
    "            except ValueError:\n",
    "                dur_ql = 1.0\n",
    "\n",
    "            if pending_pitch == \"REST\":\n",
    "                r = music21.note.Rest()\n",
    "                r.quarterLength = dur_ql\n",
    "                if current_measure is not None:\n",
    "                    current_measure.append(r)\n",
    "            elif pending_pitch is not None:\n",
    "                # Parse pitch string like \"C5\", \"Bb4\", etc.\n",
    "                try:\n",
    "                    # Extract step and octave\n",
    "                    step = pending_pitch[0]\n",
    "                    octave = int(pending_pitch[-1])\n",
    "                    n = music21.note.Note()\n",
    "                    n.pitch.step = step\n",
    "                    n.pitch.octave = octave\n",
    "                    n.quarterLength = dur_ql\n",
    "                    if current_measure is not None:\n",
    "                        current_measure.append(n)\n",
    "                except Exception:\n",
    "                    pass\n",
    "            pending_pitch = None\n",
    "            i += 1\n",
    "\n",
    "        else:\n",
    "            # Unknown token — skip\n",
    "            i += 1\n",
    "\n",
    "    # Finalize last measure and part\n",
    "    if current_measure is not None and current_part is not None:\n",
    "        current_part.append(current_measure)\n",
    "    if current_part is not None:\n",
    "        score.append(current_part)\n",
    "\n",
    "    return score\n",
    "\n",
    "\n",
    "# ── Quick round-trip test ──\n",
    "print(\"Round-trip test:\")\n",
    "print(f\"  Tokenizing {test_file.name}...\")\n",
    "test_tokens_rt = tokenize(test_file)\n",
    "print(f\"  Got {len(test_tokens_rt)} tokens\")\n",
    "print(f\"  Detokenizing...\")\n",
    "reconstructed_score = detokenize(test_tokens_rt)\n",
    "print(f\"  Reconstructed score: {len(reconstructed_score.parts)} part(s)\")\n",
    "for p in reconstructed_score.parts:\n",
    "    measures = list(p.getElementsByClass(music21.stream.Measure))\n",
    "    notes = list(p.recurse().notes)\n",
    "    print(f\"    Part '{p.partName}': {len(measures)} measures, {len(notes)} notes/rests\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e6fa894",
   "metadata": {},
   "source": [
    "## 7. Evaluation Harness\n",
    "\n",
    "Three evaluation axes:\n",
    "1. **Note-level accuracy** — compare (pitch, duration, onset) tuples from original XML vs round-trip reconstruction\n",
    "2. **Measure-duration integrity** — do measures sum to the correct total duration?\n",
    "3. **Sequence edit distance** — Levenshtein distance between original and round-trip token sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb96cd2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Evaluation Functions ──\n",
    "\n",
    "def extract_note_events_from_xml(xml_path):\n",
    "    \"\"\"Parse original MusicXML with lxml and extract note events.\n",
    "\n",
    "    Returns list of dicts: {'pitch': str, 'duration': float, 'onset': float, 'is_rest': bool}\n",
    "    Each event is (pitch_or_rest, quarterLength, onset_in_quarter_notes).\n",
    "    \"\"\"\n",
    "    tree = etree.parse(str(xml_path))\n",
    "    root = tree.getroot()\n",
    "    events = []\n",
    "\n",
    "    for part_elem in root.findall(\"part\"):\n",
    "        divisions = 1\n",
    "        pos_abs = 0.0\n",
    "\n",
    "        for measure in part_elem.findall(\"measure\"):\n",
    "            # Check for divisions update\n",
    "            for attrs in measure.findall(\"attributes\"):\n",
    "                div_text = attrs.findtext(\"divisions\")\n",
    "                if div_text:\n",
    "                    divisions = int(div_text)\n",
    "\n",
    "            for elem in measure:\n",
    "                if elem.tag == \"note\":\n",
    "                    if is_grace_note(elem):\n",
    "                        continue\n",
    "\n",
    "                    dur_ql = extract_duration_ql(elem, divisions)\n",
    "\n",
    "                    if is_rest(elem):\n",
    "                        events.append({\n",
    "                            'pitch': 'REST',\n",
    "                            'duration': dur_ql,\n",
    "                            'onset': round(pos_abs, 6),\n",
    "                            'is_rest': True,\n",
    "                        })\n",
    "                    else:\n",
    "                        pitch_elem = elem.find(\"pitch\")\n",
    "                        if pitch_elem is not None:\n",
    "                            step = pitch_elem.findtext(\"step\", \"C\")\n",
    "                            octave = pitch_elem.findtext(\"octave\", \"4\")\n",
    "                            pitch_str = f\"{step}{octave}\"\n",
    "                            events.append({\n",
    "                                'pitch': pitch_str,\n",
    "                                'duration': dur_ql,\n",
    "                                'onset': round(pos_abs, 6),\n",
    "                                'is_rest': False,\n",
    "                            })\n",
    "\n",
    "                    pos_abs += dur_ql\n",
    "\n",
    "                elif elem.tag == \"forward\":\n",
    "                    dur_text = elem.findtext(\"duration\")\n",
    "                    if dur_text:\n",
    "                        pos_abs += int(dur_text) / divisions\n",
    "\n",
    "                elif elem.tag == \"backup\":\n",
    "                    dur_text = elem.findtext(\"duration\")\n",
    "                    if dur_text:\n",
    "                        pos_abs -= int(dur_text) / divisions\n",
    "\n",
    "    return events\n",
    "\n",
    "\n",
    "def extract_note_events_from_score(score):\n",
    "    \"\"\"Extract note events from a music21.Score.\n",
    "\n",
    "    Returns list of dicts matching extract_note_events_from_xml format.\n",
    "    \"\"\"\n",
    "    events = []\n",
    "    for part in score.parts:\n",
    "        for note_or_rest in part.recurse().notesAndRests:\n",
    "            if isinstance(note_or_rest, music21.note.Rest):\n",
    "                events.append({\n",
    "                    'pitch': 'REST',\n",
    "                    'duration': note_or_rest.quarterLength,\n",
    "                    'onset': round(float(note_or_rest.offset), 6),\n",
    "                    'is_rest': True,\n",
    "                })\n",
    "            elif isinstance(note_or_rest, music21.note.Note):\n",
    "                step = note_or_rest.pitch.step\n",
    "                octave = note_or_rest.pitch.octave\n",
    "                events.append({\n",
    "                    'pitch': f\"{step}{octave}\",\n",
    "                    'duration': note_or_rest.quarterLength,\n",
    "                    'onset': round(float(note_or_rest.offset), 6),\n",
    "                    'is_rest': False,\n",
    "                })\n",
    "    return events\n",
    "\n",
    "\n",
    "def compute_note_accuracy(original_events, reconstructed_events):\n",
    "    \"\"\"Compute note-level accuracy between original and reconstructed events.\n",
    "\n",
    "    Returns dict with pitch_accuracy, duration_accuracy, onset_accuracy, combined_accuracy.\n",
    "    \"\"\"\n",
    "    n_orig = len(original_events)\n",
    "    n_recon = len(reconstructed_events)\n",
    "    n_compare = min(n_orig, n_recon)\n",
    "\n",
    "    if n_compare == 0:\n",
    "        return {\n",
    "            'pitch_accuracy': 0.0,\n",
    "            'duration_accuracy': 0.0,\n",
    "            'combined_accuracy': 0.0,\n",
    "            'n_original': n_orig,\n",
    "            'n_reconstructed': n_recon,\n",
    "            'length_match': n_orig == n_recon,\n",
    "        }\n",
    "\n",
    "    pitch_match = 0\n",
    "    dur_match = 0\n",
    "    combined_match = 0\n",
    "\n",
    "    for orig, recon in zip(original_events, reconstructed_events):\n",
    "        p_match = orig['pitch'] == recon['pitch']\n",
    "        d_match = abs(orig['duration'] - recon['duration']) < 0.01\n",
    "        if p_match:\n",
    "            pitch_match += 1\n",
    "        if d_match:\n",
    "            dur_match += 1\n",
    "        if p_match and d_match:\n",
    "            combined_match += 1\n",
    "\n",
    "    return {\n",
    "        'pitch_accuracy': pitch_match / n_compare,\n",
    "        'duration_accuracy': dur_match / n_compare,\n",
    "        'combined_accuracy': combined_match / n_compare,\n",
    "        'n_original': n_orig,\n",
    "        'n_reconstructed': n_recon,\n",
    "        'length_match': n_orig == n_recon,\n",
    "    }\n",
    "\n",
    "\n",
    "def check_measure_durations(tokens):\n",
    "    \"\"\"Check if each measure's notes sum to the expected duration from the time signature.\n",
    "\n",
    "    Returns dict with total_measures, correct_measures, integrity_pct.\n",
    "    \"\"\"\n",
    "    # Parse time signature and measure durations from tokens\n",
    "    current_bar_dur = 4.0  # default 4/4\n",
    "    measure_results = []\n",
    "    current_measure_dur = 0.0\n",
    "    in_measure = False\n",
    "    measure_num = None\n",
    "\n",
    "    for token in tokens:\n",
    "        if token.startswith(\"TIME_SIG_\"):\n",
    "            ts_str = token[9:]\n",
    "            try:\n",
    "                num, denom = ts_str.split(\"/\")\n",
    "                current_bar_dur = float(num) * (4.0 / float(denom))\n",
    "            except ValueError:\n",
    "                pass\n",
    "\n",
    "        elif token.startswith(\"BAR_\"):\n",
    "            # Save previous measure\n",
    "            if in_measure and measure_num is not None:\n",
    "                measure_results.append({\n",
    "                    'measure': measure_num,\n",
    "                    'expected': current_bar_dur,\n",
    "                    'actual': round(current_measure_dur, 6),\n",
    "                    'correct': abs(current_measure_dur - current_bar_dur) < 0.01,\n",
    "                })\n",
    "            measure_num = token[4:]\n",
    "            current_measure_dur = 0.0\n",
    "            in_measure = True\n",
    "\n",
    "        elif token.startswith(\"DUR_\"):\n",
    "            try:\n",
    "                dur = float(token[4:])\n",
    "                current_measure_dur += dur\n",
    "            except ValueError:\n",
    "                pass\n",
    "\n",
    "    # Save last measure\n",
    "    if in_measure and measure_num is not None:\n",
    "        measure_results.append({\n",
    "            'measure': measure_num,\n",
    "            'expected': current_bar_dur,\n",
    "            'actual': round(current_measure_dur, 6),\n",
    "            'correct': abs(current_measure_dur - current_bar_dur) < 0.01,\n",
    "        })\n",
    "\n",
    "    total = len(measure_results)\n",
    "    correct = sum(1 for m in measure_results if m['correct'])\n",
    "\n",
    "    return {\n",
    "        'total_measures': total,\n",
    "        'correct_measures': correct,\n",
    "        'integrity_pct': (correct / total * 100) if total > 0 else 0.0,\n",
    "        'details': measure_results,\n",
    "    }\n",
    "\n",
    "\n",
    "def levenshtein_distance(seq1, seq2):\n",
    "    \"\"\"Compute Levenshtein edit distance between two sequences.\"\"\"\n",
    "    n, m = len(seq1), len(seq2)\n",
    "    if n == 0:\n",
    "        return m\n",
    "    if m == 0:\n",
    "        return n\n",
    "\n",
    "    # Use two-row optimization for memory\n",
    "    prev = list(range(m + 1))\n",
    "    curr = [0] * (m + 1)\n",
    "\n",
    "    for i in range(1, n + 1):\n",
    "        curr[0] = i\n",
    "        for j in range(1, m + 1):\n",
    "            cost = 0 if seq1[i - 1] == seq2[j - 1] else 1\n",
    "            curr[j] = min(\n",
    "                curr[j - 1] + 1,       # insertion\n",
    "                prev[j] + 1,           # deletion\n",
    "                prev[j - 1] + cost     # substitution\n",
    "            )\n",
    "        prev, curr = curr, prev\n",
    "\n",
    "    return prev[m]\n",
    "\n",
    "\n",
    "def evaluate_file(xml_path):\n",
    "    \"\"\"Run full evaluation pipeline on a single file.\n",
    "\n",
    "    Returns dict with all metrics.\n",
    "    \"\"\"\n",
    "    result = {'file': str(xml_path), 'filename': Path(xml_path).name}\n",
    "\n",
    "    try:\n",
    "        # 1. Tokenize\n",
    "        tokens = tokenize(xml_path)\n",
    "        result['n_tokens'] = len(tokens)\n",
    "\n",
    "        # 2. Detokenize\n",
    "        recon_score = detokenize(tokens)\n",
    "\n",
    "        # 3. Extract note events from original XML\n",
    "        orig_events = extract_note_events_from_xml(xml_path)\n",
    "\n",
    "        # 4. Extract note events from reconstructed score\n",
    "        recon_events = extract_note_events_from_score(recon_score)\n",
    "\n",
    "        # 5. Note-level accuracy\n",
    "        accuracy = compute_note_accuracy(orig_events, recon_events)\n",
    "        result.update(accuracy)\n",
    "\n",
    "        # 6. Measure duration integrity\n",
    "        measure_check = check_measure_durations(tokens)\n",
    "        result['measure_integrity_pct'] = measure_check['integrity_pct']\n",
    "        result['total_measures'] = measure_check['total_measures']\n",
    "        result['correct_measures'] = measure_check['correct_measures']\n",
    "\n",
    "        # 7. Round-trip token edit distance\n",
    "        # Re-tokenize the reconstructed score by writing to temp XML and re-parsing\n",
    "        # For v0, we skip this (expensive) — we'll use note-level accuracy instead\n",
    "        result['error'] = None\n",
    "\n",
    "    except Exception as e:\n",
    "        result['error'] = str(e)\n",
    "        result['n_tokens'] = 0\n",
    "        result['pitch_accuracy'] = 0.0\n",
    "        result['duration_accuracy'] = 0.0\n",
    "        result['combined_accuracy'] = 0.0\n",
    "        result['measure_integrity_pct'] = 0.0\n",
    "        result['n_original'] = 0\n",
    "        result['n_reconstructed'] = 0\n",
    "        result['length_match'] = False\n",
    "        result['total_measures'] = 0\n",
    "        result['correct_measures'] = 0\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "def evaluate_batch(file_paths, verbose=True):\n",
    "    \"\"\"Run evaluation on a batch of files.\n",
    "\n",
    "    Returns a pandas DataFrame with per-file results.\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    errors = []\n",
    "\n",
    "    for i, fp in enumerate(file_paths):\n",
    "        if verbose and (i + 1) % 20 == 0:\n",
    "            print(f\"  Evaluating file {i + 1}/{len(file_paths)}...\")\n",
    "        r = evaluate_file(fp)\n",
    "        results.append(r)\n",
    "        if r['error']:\n",
    "            errors.append((fp, r['error']))\n",
    "\n",
    "    df = pd.DataFrame(results)\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"\\nEvaluation complete: {len(results)} files\")\n",
    "        if errors:\n",
    "            print(f\"  Errors: {len(errors)} files failed\")\n",
    "            for fp, err in errors[:5]:\n",
    "                print(f\"    {Path(fp).name}: {err}\")\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "print(\"Evaluation functions defined successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ae80723",
   "metadata": {},
   "source": [
    "## 8. Run Evaluation on 100-File Sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76438920",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Run evaluation on 100-file sample ──\n",
    "print(f\"Evaluating {len(eval_sample)} files...\\n\")\n",
    "eval_df = evaluate_batch(eval_sample)\n",
    "\n",
    "# Filter to successful evaluations\n",
    "eval_ok = eval_df[eval_df['error'].isna()].copy()\n",
    "print(f\"\\nSuccessful evaluations: {len(eval_ok)} / {len(eval_df)}\")\n",
    "\n",
    "# ── Summary statistics ──\n",
    "metrics = ['pitch_accuracy', 'duration_accuracy', 'combined_accuracy', 'measure_integrity_pct']\n",
    "summary = eval_ok[metrics].describe().T\n",
    "summary['median'] = eval_ok[metrics].median()\n",
    "print(\"\\n╔══════════════════════════════════════════════════════════╗\")\n",
    "print(\"║          v0 EVALUATION SUMMARY (100-file sample)        ║\")\n",
    "print(\"╠══════════════════════════════════════════════════════════╣\")\n",
    "print(summary[['mean', 'median', 'min', 'max', 'std']].to_string())\n",
    "print(\"╚══════════════════════════════════════════════════════════╝\")\n",
    "\n",
    "# Perfect reconstruction count\n",
    "perfect = len(eval_ok[eval_ok['combined_accuracy'] == 1.0])\n",
    "print(f\"\\nPerfect reconstruction (combined=1.0): {perfect}/{len(eval_ok)} ({perfect/len(eval_ok)*100:.1f}%)\")\n",
    "\n",
    "# Length match stats\n",
    "length_match = eval_ok['length_match'].sum()\n",
    "print(f\"Length match (same # events): {length_match}/{len(eval_ok)} ({length_match/len(eval_ok)*100:.1f}%)\")\n",
    "\n",
    "# Token stats\n",
    "print(f\"\\nTotal tokens across sample: {eval_ok['n_tokens'].sum():,}\")\n",
    "print(f\"Mean tokens per file: {eval_ok['n_tokens'].mean():.0f}\")\n",
    "\n",
    "# Collect unique tokens for vocabulary size\n",
    "all_tokens_sample = []\n",
    "for fp in eval_sample[:20]:  # subset for speed\n",
    "    try:\n",
    "        all_tokens_sample.extend(tokenize(fp))\n",
    "    except:\n",
    "        pass\n",
    "vocab = set(all_tokens_sample)\n",
    "print(f\"Vocabulary size (from 20-file subset): {len(vocab)}\")\n",
    "\n",
    "# Save v0 results for later comparison\n",
    "v0_results = {\n",
    "    'pitch_accuracy_mean': eval_ok['pitch_accuracy'].mean(),\n",
    "    'duration_accuracy_mean': eval_ok['duration_accuracy'].mean(),\n",
    "    'combined_accuracy_mean': eval_ok['combined_accuracy'].mean(),\n",
    "    'measure_integrity_mean': eval_ok['measure_integrity_pct'].mean(),\n",
    "    'perfect_reconstruction_pct': perfect / len(eval_ok) * 100 if len(eval_ok) > 0 else 0,\n",
    "}\n",
    "print(f\"\\nv0 results saved for comparison with future versions.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25430747",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Visualizations ──\n",
    "fig, axes = plt.subplots(1, 3, figsize=(16, 4))\n",
    "\n",
    "# Pitch accuracy histogram\n",
    "axes[0].hist(eval_ok['pitch_accuracy'], bins=20, edgecolor='black', alpha=0.7, color='steelblue')\n",
    "axes[0].set_title('Pitch Accuracy Distribution')\n",
    "axes[0].set_xlabel('Pitch Accuracy')\n",
    "axes[0].set_ylabel('Count')\n",
    "axes[0].axvline(eval_ok['pitch_accuracy'].mean(), color='red', linestyle='--', label=f\"mean={eval_ok['pitch_accuracy'].mean():.3f}\")\n",
    "axes[0].legend()\n",
    "\n",
    "# Duration accuracy histogram\n",
    "axes[1].hist(eval_ok['duration_accuracy'], bins=20, edgecolor='black', alpha=0.7, color='seagreen')\n",
    "axes[1].set_title('Duration Accuracy Distribution')\n",
    "axes[1].set_xlabel('Duration Accuracy')\n",
    "axes[1].axvline(eval_ok['duration_accuracy'].mean(), color='red', linestyle='--', label=f\"mean={eval_ok['duration_accuracy'].mean():.3f}\")\n",
    "axes[1].legend()\n",
    "\n",
    "# Measure integrity histogram\n",
    "axes[2].hist(eval_ok['measure_integrity_pct'], bins=20, edgecolor='black', alpha=0.7, color='goldenrod')\n",
    "axes[2].set_title('Measure Duration Integrity (%)')\n",
    "axes[2].set_xlabel('% Correct Measures')\n",
    "axes[2].axvline(eval_ok['measure_integrity_pct'].mean(), color='red', linestyle='--', label=f\"mean={eval_ok['measure_integrity_pct'].mean():.1f}%\")\n",
    "axes[2].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.suptitle('v0 Baseline Evaluation', y=1.02, fontsize=14, fontweight='bold')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9412f743",
   "metadata": {},
   "source": [
    "## 9. Inspect Worst-Scoring Files and Error Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "406573db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Error analysis: inspect worst-scoring files ──\n",
    "worst_files = eval_ok.nsmallest(5, 'combined_accuracy')\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"TOP 5 WORST-SCORING FILES (by combined accuracy)\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "for _, row in worst_files.iterrows():\n",
    "    fp = row['file']\n",
    "    print(f\"\\n{'─' * 60}\")\n",
    "    print(f\"File: {row['filename']}\")\n",
    "    print(f\"  Pitch accuracy:    {row['pitch_accuracy']:.4f}\")\n",
    "    print(f\"  Duration accuracy: {row['duration_accuracy']:.4f}\")\n",
    "    print(f\"  Combined accuracy: {row['combined_accuracy']:.4f}\")\n",
    "    print(f\"  Measure integrity: {row['measure_integrity_pct']:.1f}%\")\n",
    "    print(f\"  Notes: {row['n_original']} original → {row['n_reconstructed']} reconstructed\")\n",
    "    print(f\"  Length match: {row['length_match']}\")\n",
    "\n",
    "    # Show first 50 tokens\n",
    "    try:\n",
    "        tokens = tokenize(fp)\n",
    "        print(f\"\\n  First 30 tokens:\")\n",
    "        for i, t in enumerate(tokens[:30]):\n",
    "            print(f\"    [{i:3d}] {t}\")\n",
    "    except Exception as e:\n",
    "        print(f\"  Error re-tokenizing: {e}\")\n",
    "\n",
    "# ── Categorize error patterns across all evaluated files ──\n",
    "print(\"\\n\\n\" + \"=\" * 70)\n",
    "print(\"ERROR PATTERN ANALYSIS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "error_categories = Counter()\n",
    "for _, row in eval_ok.iterrows():\n",
    "    if not row['length_match']:\n",
    "        error_categories['length_mismatch'] += 1\n",
    "    if row['pitch_accuracy'] < 1.0:\n",
    "        error_categories['pitch_errors'] += 1\n",
    "    if row['duration_accuracy'] < 1.0:\n",
    "        error_categories['duration_errors'] += 1\n",
    "    if row['measure_integrity_pct'] < 100.0:\n",
    "        error_categories['measure_integrity_errors'] += 1\n",
    "\n",
    "print(f\"\\n  Total files evaluated: {len(eval_ok)}\")\n",
    "for cat, count in error_categories.most_common():\n",
    "    print(f\"  {cat}: {count}/{len(eval_ok)} ({count/len(eval_ok)*100:.1f}%)\")\n",
    "\n",
    "# ── Analyze which features cause issues ──\n",
    "print(\"\\n\\nKnown v0 limitations causing errors:\")\n",
    "print(\"  1. Microtonal <alter> values ignored → pitch tokens lose inflection\")\n",
    "print(\"     (pitch accuracy measures step+octave only, so this won't show yet)\")\n",
    "print(\"  2. Grace notes skipped → may cause position/count mismatches\")\n",
    "print(\"  3. No tie tokens → tied notes counted as separate events\")\n",
    "print(\"  4. Pickup measures (anacrusis) → measure duration != time sig\")\n",
    "print(\"  5. Repeats and endings → not tokenized, may affect measure counting\")\n",
    "\n",
    "# Check for grace notes in the sample\n",
    "grace_note_files = 0\n",
    "for fp in eval_sample:\n",
    "    try:\n",
    "        tree = etree.parse(str(fp))\n",
    "        root = tree.getroot()\n",
    "        graces = root.findall(\".//grace\")\n",
    "        if len(graces) > 0:\n",
    "            grace_note_files += 1\n",
    "    except:\n",
    "        pass\n",
    "print(f\"\\n  Files with grace notes in sample: {grace_note_files}/{len(eval_sample)}\")\n",
    "\n",
    "# Check for ties\n",
    "tie_files = 0\n",
    "for fp in eval_sample:\n",
    "    try:\n",
    "        tree = etree.parse(str(fp))\n",
    "        root = tree.getroot()\n",
    "        ties = root.findall(\".//tie\")\n",
    "        if len(ties) > 0:\n",
    "            tie_files += 1\n",
    "    except:\n",
    "        pass\n",
    "print(f\"  Files with ties in sample: {tie_files}/{len(eval_sample)}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
